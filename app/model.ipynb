{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### The Johns Hopkins University: Building Machine Models to Predict Sepsis\n",
    "\n",
    "## Business Understanding:\n",
    "Goal: To develop a predictive model that accurately identifies patients at risk of developing sepsis.\n",
    "\n",
    "Null Hypothesis (H₀): There is no significant relationship between the patient's medical data and the likelihood of developing sepsis.\n",
    "\n",
    "Alternative Hypothesis (H₁): There is a significant relationship between the patient's medical data and the likelihood of developing sepsis.\n",
    "\n",
    "#### Analytical questions:\n",
    "1. What is the correlationship between age and sepsis?\n",
    "2. What effect does the level of plasma glucose have on sepsis?\n",
    "3. What is the effect of blood pressure on sepsis?\n",
    "4. What is the relationship between body mass index and sepsis?\n",
    "5. Does one insurance health affect sepsis?\n",
    "\n",
    "### Features\n",
    "1. ID: number to represent patient ID\n",
    "2. PRG: Plasma glucose\n",
    "3. PL: Blood Work Result-1 (mu U/ml)\n",
    "4. PR: Blood Pressure (mm Hg)\n",
    "5. SK: Blood Work Result-2 (mm)\n",
    "6. TS: Blood Work Result-3 (mu U/ml)\n",
    "7. M11: Body mass index (weight in kg/(height in m)^2\n",
    "8. BD2: Blood Work Result-4 (mu U/ml)\n",
    "9. Age: patients age (years)\n",
    "10. Insurance: If a patient holds a valid insurance card\n",
    "11. Sepsis: Positive: if a patient in ICU will develop a sepsis , and Negative: otherwise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Manipulation and Hypothesis testing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "\n",
    "# For saving the model, processor and cleaned data sets\n",
    "import pickle\n",
    "import joblib\n",
    "\n",
    "# Visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.utils import resample\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import (\n",
    "    RobustScaler, \n",
    "    LabelEncoder, \n",
    "    QuantileTransformer\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, \n",
    "    GridSearchCV, \n",
    "    RandomizedSearchCV,\n",
    "    StratifiedKFold\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    confusion_matrix, \n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    roc_auc_score,\n",
    "    classification_report\n",
    ")\n",
    "from sklearn.ensemble import (\n",
    "    AdaBoostClassifier, \n",
    "    BaggingClassifier, \n",
    "    ExtraTreesClassifier, \n",
    "    GradientBoostingClassifier, \n",
    "    RandomForestClassifier\n",
    ")\n",
    "\n",
    "# Utilities\n",
    "import warnings\n",
    "import joblib\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "          ID  PRG   PL  PR  SK   TS   M11    BD2  Age  Insurance   Sepssis\n",
       "0  ICU200010    6  148  72  35    0  33.6  0.627   50          0  Positive\n",
       "1  ICU200011    1   85  66  29    0  26.6  0.351   31          0  Negative\n",
       "2  ICU200012    8  183  64   0    0  23.3  0.672   32          1  Positive\n",
       "3  ICU200013    1   89  66  23   94  28.1  0.167   21          1  Negative\n",
       "4  ICU200014    0  137  40  35  168  43.1  2.288   33          1  Positive\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import the train data\n",
    "train_data = pd.read_csv(r'C:\\Users\\HP\\OneDrive\\Desktop\\API DOCKER\\data\\Paitients_Files_Train.csv')\n",
    "print(train_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "          ID  PRG   PL  PR  SK   TS   M11    BD2  Age  Insurance\n",
       "0  ICU200609    1  109  38  18  120  23.1  0.407   26          1\n",
       "1  ICU200610    1  108  88  19    0  27.1  0.400   24          1\n",
       "2  ICU200611    6   96   0   0    0  23.7  0.190   28          1\n",
       "3  ICU200612    1  124  74  36    0  27.8  0.100   30          1\n",
       "4  ICU200613    7  150  78  29  126  35.2  0.692   54          0\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import the test data\n",
    "test_data = pd.read_csv(r'C:\\Users\\HP\\OneDrive\\Desktop\\API DOCKER\\data\\Paitients_Files_Test.csv')\n",
    "print(test_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis (EDA) of Train Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'pandas.core.frame.DataFrame'>\n",
       "RangeIndex: 599 entries, 0 to 598\n",
       "Data columns (total 11 columns):\n",
       " #   Column     Non-Null Count  Dtype  \n",
       "---  ------     --------------  -----  \n",
       " 0   ID         599 non-null    object \n",
       " 1   PRG        599 non-null    int64  \n",
       " 2   PL         599 non-null    int64  \n",
       " 3   PR         599 non-null    int64  \n",
       " 4   SK         599 non-null    int64  \n",
       " 5   TS         599 non-null    int64  \n",
       " 6   M11        599 non-null    float64\n",
       " 7   BD2        599 non-null    float64\n",
       " 8   Age        599 non-null    int64  \n",
       " 9   Insurance  599 non-null    int64  \n",
       " 10  Sepssis    599 non-null    object \n",
       "dtypes: float64(2), int64(7), object(2)\n",
       "memory usage: 51.6+ KB\n",
       "None\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check the info of train data\n",
    "print(train_data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(599, 11)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check the Shape of the Train Data Set\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "              PRG          PL          PR          SK          TS         M11  \\\n",
       "count  599.000000  599.000000  599.000000  599.000000  599.000000  599.000000   \n",
       "mean     3.824708  120.153589   68.732888   20.562604   79.460768   31.920033   \n",
       "std      3.362839   32.682364   19.335675   16.017622  116.576176    8.008227   \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      1.000000   99.000000   64.000000    0.000000    0.000000   27.100000   \n",
       "50%      3.000000  116.000000   70.000000   23.000000   36.000000   32.000000   \n",
       "75%      6.000000  140.000000   80.000000   32.000000  123.500000   36.550000   \n",
       "max     17.000000  198.000000  122.000000   99.000000  846.000000   67.100000   \n",
       "\n",
       "              BD2         Age   Insurance  \n",
       "count  599.000000  599.000000  599.000000  \n",
       "mean     0.481187   33.290484    0.686144  \n",
       "std      0.337552   11.828446    0.464447  \n",
       "min      0.078000   21.000000    0.000000  \n",
       "25%      0.248000   24.000000    0.000000  \n",
       "50%      0.383000   29.000000    1.000000  \n",
       "75%      0.647000   40.000000    1.000000  \n",
       "max      2.420000   81.000000    1.000000  \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check the five summary statistics of the train data\n",
    "train_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        count unique        top freq\n",
       "ID        599    599  ICU200010    1\n",
       "Sepssis   599      2   Negative  391\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check the five-summary statistics for category columns of the train data\n",
    "train_data.describe(include = 'object').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check for duplicates in the train data\n",
    "train_data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID           0\n",
       "PRG          0\n",
       "PL           0\n",
       "PR           0\n",
       "SK           0\n",
       "TS           0\n",
       "M11          0\n",
       "BD2          0\n",
       "Age          0\n",
       "Insurance    0\n",
       "Sepssis      0\n",
       "dtype: int64\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check for missing values in the train data\n",
    "train_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  Patient_ID  Plasma_Glucose  Blood_Work_Result-1 (mu U/ml)  \\\n",
       "0  ICU200010               6                            148   \n",
       "1  ICU200011               1                             85   \n",
       "2  ICU200012               8                            183   \n",
       "3  ICU200013               1                             89   \n",
       "4  ICU200014               0                            137   \n",
       "\n",
       "   Blood_Pressure (mm Hg)  Blood_Work_Result-2 (mm)  \\\n",
       "0                      72                        35   \n",
       "1                      66                        29   \n",
       "2                      64                         0   \n",
       "3                      66                        23   \n",
       "4                      40                        35   \n",
       "\n",
       "   Blood_Work_Result-3 (mu U/ml)  \\\n",
       "0                              0   \n",
       "1                              0   \n",
       "2                              0   \n",
       "3                             94   \n",
       "4                            168   \n",
       "\n",
       "   Body_Mass_Index (weight in kg/(height in m)^2  \\\n",
       "0                                           33.6   \n",
       "1                                           26.6   \n",
       "2                                           23.3   \n",
       "3                                           28.1   \n",
       "4                                           43.1   \n",
       "\n",
       "   Blood_Work_Result-4 (mu U/ml)  Age  Insurance    Sepsis  \n",
       "0                          0.627   50          0  Positive  \n",
       "1                          0.351   31          0  Negative  \n",
       "2                          0.672   32          1  Positive  \n",
       "3                          0.167   21          1  Negative  \n",
       "4                          2.288   33          1  Positive  \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Rename the columns to give descriptive meaning\n",
    "\n",
    "cols = {\n",
    "    'ID': 'Patient_ID',\n",
    "    'PRG': 'Plasma_Glucose',\n",
    "    'PL': 'Blood_Work_Result-1 (mu U/ml)',\n",
    "    'PR': 'Blood_Pressure (mm Hg)',\n",
    "    'SK': 'Blood_Work_Result-2 (mm)',\n",
    "    'TS': 'Blood_Work_Result-3 (mu U/ml)',\n",
    "    'M11': 'Body_Mass_Index (weight in kg/(height in m)^2',\n",
    "    'BD2': 'Blood_Work_Result-4 (mu U/ml)',\n",
    "    'Sepssis' : 'Sepsis'\n",
    "}\n",
    "\n",
    "train_data.rename(columns = cols, inplace = True)\n",
    "\n",
    "# Check the data\n",
    "print(train_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A histgram plot showing the distribution of numerical columns\n",
    "\n",
    "train_data.hist(\n",
    "    figsize = (12, 10),\n",
    "    color = 'skyblue',\n",
    "    grid = False,\n",
    "    bins = 15\n",
    ")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for outliers by ploting the box plot\n",
    "\n",
    "plt.figure(figsize = (10, 8))\n",
    "sns.boxplot(\n",
    "    data = train_data,\n",
    "    orient = 'h'\n",
    ")\n",
    "plt.xscale('log') # Some features have extreme large numbers\n",
    "plt.title(\"Outlier Features\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for kde distribution\n",
    "\n",
    "sns.kdeplot(\n",
    "    data = train_data\n",
    ")\n",
    "plt.xscale('log') # Some features have extreme large values\n",
    "plt.title('KDE Distribution of Numerical Columns')\n",
    "plt.xlabel('Values')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking whether our target variable is balanced using count plot\n",
    "\n",
    "ax = sns.countplot(\n",
    "    data = train_data,\n",
    "    x = 'Sepsis'\n",
    ")\n",
    "plt.xlabel('Class Target')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title(\"A Count plot of the Class Target\")\n",
    "\n",
    "# Annotate the figures for each category\n",
    "for p in ax.patches:\n",
    "    ax.annotate(\n",
    "        f\"{int(p.get_height())}\",\n",
    "        (p.get_x() + p.get_width()/2, p.get_height()),\n",
    "        ha = 'center',\n",
    "        va = 'center'\n",
    "    )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Patient_ID', 'Plasma_Glucose', 'Blood_Work_Result-1 (mu U/ml)',\n",
       "       'Blood_Pressure (mm Hg)', 'Blood_Work_Result-2 (mm)',\n",
       "       'Blood_Work_Result-3 (mu U/ml)',\n",
       "       'Body_Mass_Index (weight in kg/(height in m)^2',\n",
       "       'Blood_Work_Result-4 (mu U/ml)', 'Age', 'Insurance', 'Sepsis'],\n",
       "      dtype='object')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check the columns for the train data set\n",
    "train_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bivariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatterplot of Age vs Plasma Glucose\n",
    "sns.scatterplot(\n",
    "    data = train_data,\n",
    "    x = 'Age',\n",
    "    y = 'Plasma_Glucose'\n",
    ")\n",
    "plt.title('Relationship Between Age and Plasma Glucose')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A bar chart of Sepsis vs Blood_Work_Result-1 (mu U/ml)\n",
    "\n",
    "ax = sns.barplot(\n",
    "    data = train_data,\n",
    "    x = 'Sepsis',\n",
    "    y = 'Blood_Work_Result-1 (mu U/ml)',\n",
    "    ci = None\n",
    ")\n",
    "plt.title(\"A Barchart of Sepsis vs Blood_Work_Result-1 (mu U/ml)\")\n",
    "\n",
    "# Annotate the values for each bar\n",
    "for p in ax.patches:\n",
    "    ax.annotate(\n",
    "        f\"{int(p.get_height())}\",\n",
    "        (p.get_x() + p.get_width() / 2, p.get_height()),\n",
    "         ha = 'center',\n",
    "        va = 'center'\n",
    "    )\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-variate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the numerical columns\n",
    "\n",
    "numerical_col = train_data.select_dtypes(exclude = 'object').columns\n",
    "\n",
    "# Create a data frame for numerical columns only\n",
    "\n",
    "data_num_col = train_data[numerical_col]\n",
    "\n",
    "# Calculate the correlationship for the numrical data frame\n",
    "\n",
    "corr = data_num_col.corr()\n",
    "\n",
    "# Visualize the correlationship using heatmap\n",
    "\n",
    "plt.figure(figsize = (12, 10))\n",
    "sns.heatmap(corr, annot = True, cmap = 'coolwarm', fmt = '.2f')\n",
    "plt.title(\"Correlationship Between Features\", fontsize = 12)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bivariate Analaysis using Pairplot, Features vs Target\n",
    "\n",
    "plt.figure(figsize = (12, 10))\n",
    "sns.pairplot(\n",
    "    data = train_data,\n",
    "    diag_kind = 'kde',\n",
    "    hue = 'Sepsis'\n",
    ")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analytical Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "From the graph its clear that patients who are older are likely to have sepsis\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# i. What is the correlationship between age and sepsis?\n",
    "ax = sns.barplot(\n",
    "    data = train_data,\n",
    "    x = 'Sepsis',\n",
    "    y = 'Age',\n",
    "    ci = None\n",
    ")\n",
    "plt.title('A Barchart of Sepsis vs Age')\n",
    "# Annotate\n",
    "for p in ax.patches:\n",
    "    ax.annotate(\n",
    "        f\"{int(p.get_height())}\",\n",
    "        (p.get_x() + p.get_width() /2, p.get_height()),\n",
    "        ha = 'center',\n",
    "        va = 'center'\n",
    "    )\n",
    "plt.show();\n",
    "\n",
    "print(f\"From the graph its clear that patients who are older are likely to have sepsis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "From the boxplot, pateints with higher plasma glucose are more likely to have sepsis\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ii. What effect does the level of plasma glucose have on sepsis?\n",
    "\n",
    "sns.boxplot(\n",
    "    data = train_data,\n",
    "    x = 'Plasma_Glucose',\n",
    "    y = 'Sepsis'\n",
    ")\n",
    "plt.show();\n",
    "\n",
    "print(f\"From the boxplot, pateints with higher plasma glucose are more likely to have sepsis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iii. What is the effect of blood pressure on sepsis?\n",
    "ax=sns.barplot(\n",
    "    data = train_data,\n",
    "    x = 'Sepsis',\n",
    "    y = 'Blood_Pressure (mm Hg)',\n",
    "    ci = None\n",
    ")\n",
    "for p in ax.patches:\n",
    "    ax.annotate(\n",
    "        f\"{int(p.get_height())}\",\n",
    "        (p.get_x() + p.get_width() / 2, p.get_height()),\n",
    "        ha = 'center',\n",
    "        va = 'center'\n",
    "    )\n",
    "plt.title('Effect on blood pressure')\n",
    "plt.xlabel('Sepsis')\n",
    "plt.ylabel('Blood Pressure')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iv. What is the relationship between body mass index and sepsis?\n",
    "ax=sns.barplot(\n",
    "    data = train_data,\n",
    "    x = 'Sepsis',\n",
    "    y = 'Body_Mass_Index (weight in kg/(height in m)^2',\n",
    "    ci = None\n",
    ")\n",
    "for p in ax.patches:\n",
    "    ax.annotate(\n",
    "        f\"{int(p.get_height())}\",\n",
    "        (p.get_x() + p.get_width() / 2, p.get_height()),\n",
    "        ha = 'center',\n",
    "        va = 'center'\n",
    "    )\n",
    "plt.title('The relationship Between Body Mass on Sepsis')\n",
    "plt.xlabel('Sepsis')\n",
    "plt.ylabel('Body Mass')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v. Does one insurance health affect sepsis?\n",
    "ax = sns.barplot(\n",
    "    data = train_data,\n",
    "    x = 'Sepsis',\n",
    "    y = 'Insurance',\n",
    "    ci = None\n",
    ")\n",
    "plt.title('Insurance vs Sepsis')\n",
    "plt.xlabel('Sepsis')\n",
    "plt.ylabel('Insurance')\n",
    "\n",
    "for p in ax.patches:\n",
    "    ax.annotate(\n",
    "        f\"{round(p.get_height(), 3)}\",\n",
    "        (p.get_x() + p.get_width() / 2, p.get_height()),\n",
    "        ha = 'center',\n",
    "        va = 'center'\n",
    "    )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.applymap(lambda x: x.encode('utf-8', 'ignore').decode('utf-8') if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Null Hypothesis (H₀): There is no significant relationship between the patient's medical data and the likelihood of developing sepsis.\n",
    "# Alternative Hypothesis (H₁): There is a significant relationship between the patient's medical data and the likelihood of developing sepsis.\n",
    "\n",
    "# Set the alpha value\n",
    "\n",
    "alpha = 0.05\n",
    "\n",
    "# Split the data into features and target\n",
    "features = train_data[['Body_Mass_Index (weight in kg/(height in m)^2']]\n",
    "\n",
    "# Get the target variable and convert it numerical\n",
    "target = train_data[['Sepsis']]\n",
    "\n",
    "# Intialize label encoder\n",
    "le = LabelEncoder()\n",
    "target = le.fit_transform(target)\n",
    "\n",
    "\n",
    "# Get spearmanr correlation and p value of features vs target\n",
    "corr, p_value = spearmanr(features, target)\n",
    "print(f\"The spearmanr p value of the data is: {p_value}\")\n",
    "\n",
    "# Decision making based on p-value\n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis (H₀): There is no significant relationship between the patient's medical data and the likelihood of developing sepsis.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis (H₀): There is a significant relationship between the patient's medical data and the likelihood of developing sepsis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Insights\n",
    "1. There are no missing values in the dataset\n",
    "2. There are no duplicate values in the data set\n",
    "3. The Insurance has well distributed values with a minimum of zero (0) and a maximum of one (1)\n",
    "4. All the features have a minimun value of zero (0)\n",
    "5. The blood work result-3 has the greatest outlier with minimun value of zero (0) and a maximum value of 846\n",
    "6. Reject the Null Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     Sepsis  Count_Sepsis\n",
       "0  Negative           391\n",
       "1  Positive           208\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check is the data is balanced and visualize\n",
    "\n",
    "class_sepsis = train_data['Sepsis'].value_counts().rename('Count_Sepsis').reset_index()                                               \n",
    "print(class_sepsis) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Visual of the target Class Balance\n",
    "\n",
    "ax = sns.countplot(\n",
    "    data = train_data,\n",
    "    x = 'Sepsis',\n",
    "    palette = 'viridis'\n",
    ")\n",
    "plt.xlabel('Class Target')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Count of Class Target')\n",
    "\n",
    "# Annotate the figures for each category\n",
    "for p in ax.patches:\n",
    "    ax.annotate(\n",
    "        f\"{int(p.get_height())}\",\n",
    "        (p.get_x() + p.get_width()/2, p.get_height()),\n",
    "        ha = 'center',\n",
    "        va = 'center'\n",
    "    )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sepsis\n",
       "Negative    391\n",
       "Positive    391\n",
       "Name: count, dtype: int64\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# From the analysis above, its clear the positive is minority class\n",
    "\n",
    "minority_sepsis = train_data[train_data['Sepsis'] == 'Positive']\n",
    "majority_sepsis = train_data[train_data['Sepsis'] == 'Negative']\n",
    "\n",
    "# Resample the minority class to match the majority class size\n",
    "minor_resampled = resample(minority_sepsis, replace=True, n_samples=len(majority_sepsis), random_state=42)\n",
    "\n",
    "# Combine the resampled minority class with the majority class\n",
    "train_model = pd.concat([majority_sepsis, minor_resampled])\n",
    "\n",
    "# Check the class distribution in the resampled dataset\n",
    "print(train_model['Sepsis'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the Data Set into Features and Target Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the feature variables\n",
    "\n",
    "X = train_model.drop(columns = ['Patient_ID', 'Sepsis'], axis = 1)\n",
    "\n",
    "# Get the target variable and convert it into numeric\n",
    "\n",
    "y = train_model['Sepsis'].apply(lambda x: 1 if x == 'Negative' else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the Data Set into Train and Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and validation set\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, stratify = y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(((625, 9), (625,)), ((157, 9), (157,)))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#check the size of the split data\n",
    "\n",
    "(X_train.shape, y_train.shape), (X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Plasma_Glucose', 'Blood_Work_Result-1 (mu U/ml)',\n",
       "       'Blood_Pressure (mm Hg)', 'Blood_Work_Result-2 (mm)',\n",
       "       'Blood_Work_Result-3 (mu U/ml)',\n",
       "       'Body_Mass_Index (weight in kg/(height in m)^2',\n",
       "       'Blood_Work_Result-4 (mu U/ml)', 'Age', 'Insurance'],\n",
       "      dtype='object')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Identify the Feature Columns that will need Transformation\n",
    "\n",
    "feature_cols = X.columns\n",
    "print(feature_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ColumnTransformer(transformers=[('feature_columns',\n",
       "                                 Pipeline(steps=[('simpleimputer',\n",
       "                                                  SimpleImputer(strategy='median')),\n",
       "                                                 ('robustscaler',\n",
       "                                                  RobustScaler()),\n",
       "                                                 ('quantiletransformer',\n",
       "                                                  QuantileTransformer(output_distribution='normal'))]),\n",
       "                                 Index(['Plasma_Glucose', 'Blood_Work_Result-1 (mu U/ml)',\n",
       "       'Blood_Pressure (mm Hg)', 'Blood_Work_Result-2 (mm)',\n",
       "       'Blood_Work_Result-3 (mu U/ml)',\n",
       "       'Body_Mass_Index (weight in kg/(height in m)^2',\n",
       "       'Blood_Work_Result-4 (mu U/ml)', 'Age', 'Insurance'],\n",
       "      dtype='object'))])\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers = [\n",
    "        ('feature_columns', make_pipeline(\n",
    "            SimpleImputer(strategy = 'median'),\n",
    "            RobustScaler(),\n",
    "            QuantileTransformer(output_distribution = 'normal')),\n",
    "         feature_cols\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Classification models\n",
    "\n",
    "models = {\n",
    "    \"logistic_regression\": LogisticRegression(),\n",
    "    \"decision_tree\": DecisionTreeClassifier(),\n",
    "    \"random_forest\": RandomForestClassifier(),\n",
    "    \"gradient_boosting\": GradientBoostingClassifier(),\n",
    "    \"ada_boost\": AdaBoostClassifier(),\n",
    "    \"extra_trees\": ExtraTreesClassifier(),\n",
    "    \"K_nearest_neighbor\": KNeighborsClassifier(),\n",
    "    \"bagging_classifier\": BaggingClassifier()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_val_processed = preprocessor.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                     f1_score precision_score recall_score accuracy_score  \\\n",
       "extra_trees          0.910256        0.910256     0.910256       0.910828   \n",
       "random_forest        0.894737        0.918919     0.871795       0.898089   \n",
       "decision_tree        0.885906        0.929577     0.846154        0.89172   \n",
       "gradient_boosting    0.874172         0.90411     0.846154       0.878981   \n",
       "bagging_classifier   0.864865        0.914286     0.820513       0.872611   \n",
       "ada_boost            0.824324        0.871429     0.782051       0.834395   \n",
       "logistic_regression  0.753425        0.808824     0.705128       0.770701   \n",
       "K_nearest_neighbor   0.737589        0.825397     0.666667       0.764331   \n",
       "\n",
       "                         confusion_matrix  \\\n",
       "extra_trees            [[72, 7], [7, 71]]   \n",
       "random_forest         [[73, 6], [10, 68]]   \n",
       "decision_tree         [[74, 5], [12, 66]]   \n",
       "gradient_boosting     [[72, 7], [12, 66]]   \n",
       "bagging_classifier    [[73, 6], [14, 64]]   \n",
       "ada_boost             [[70, 9], [17, 61]]   \n",
       "logistic_regression  [[66, 13], [23, 55]]   \n",
       "K_nearest_neighbor   [[68, 11], [26, 52]]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                           predictions  \n",
       "extra_trees          [1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, ...]  \n",
       "random_forest        [1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, ...]  \n",
       "decision_tree        [1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, ...]  \n",
       "gradient_boosting    [1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, ...]  \n",
       "bagging_classifier   [1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, ...]  \n",
       "ada_boost            [1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, ...]  \n",
       "logistic_regression  [0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, ...]  \n",
       "K_nearest_neighbor   [0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, ...]  \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize an empty dictionary to score evaluation scores\n",
    "model_report = {}\n",
    "\n",
    "# Loop through models, fit, and evaluate each model\n",
    "for model_name, model_classifier in models.items():\n",
    "    \n",
    "    # Fit the model on the processed training data\n",
    "    model_classifier.fit(X_train_processed, y_train)\n",
    "    \n",
    "    # Predict on the processed test data\n",
    "    y_pred = model_classifier.predict(X_val_processed)\n",
    "    \n",
    "    # Get the evaluation score for each metric\n",
    "    F1_Score = f1_score(y_val, y_pred)\n",
    "    Precision_Score = precision_score(y_val, y_pred)\n",
    "    Recall_Score = recall_score(y_val, y_pred)\n",
    "    Accuracy_Score = accuracy_score(y_val, y_pred)\n",
    "    Confusion_Matrix = confusion_matrix(y_val, y_pred)\n",
    "    Predictions = y_pred\n",
    "    \n",
    "    # Store the evaluation score for each model\n",
    "    model_report[model_name] = {\n",
    "        'f1_score': F1_Score,\n",
    "        'precision_score': Precision_Score,\n",
    "        'recall_score': Recall_Score,\n",
    "        'accuracy_score': Accuracy_Score,\n",
    "        'confusion_matrix': Confusion_Matrix,\n",
    "        'predictions': Predictions\n",
    "    }\n",
    "\n",
    "# Convert the model_reports dictionary into a dataframe\n",
    "eval_report = pd.DataFrame(model_report).transpose()\n",
    "\n",
    "# Sort the dataframe in descending order by the f1_score column\n",
    "eval_report_df = eval_report.sort_values('f1_score', ascending = False)\n",
    "\n",
    "# Display the dataframe\n",
    "print(eval_report_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a visualization of confusion matrix\n",
    "\n",
    "def confusion_matrix_plot(confusion_matrix, model):\n",
    "    plt.figure(figsize = (5, 4))\n",
    "    sns.heatmap(\n",
    "        confusion_matrix,\n",
    "        annot = True,\n",
    "        cmap = \"coolwarm\",\n",
    "        fmt = 'd',\n",
    "        xticklabels = [\"Positive\", \"Negative\"],\n",
    "        yticklabels = [\"Positive\", \"Negative\"]\n",
    "    )\n",
    "    plt.title(f\"Confusion_Matrix for: {model}\")\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"Actual Label\")\n",
    "    \n",
    "for model in model_report.keys():\n",
    "    confusion_matrix = model_report[model][\"confusion_matrix\"]\n",
    "    confusion_matrix_plot(confusion_matrix, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Classification Report for logistic_regression\n",
       "              precision    recall  f1-score   support\n",
       "\n",
       "    Positive       0.84      0.92      0.88        79\n",
       "    Negative       0.91      0.82      0.86        78\n",
       "\n",
       "    accuracy                           0.87       157\n",
       "   macro avg       0.88      0.87      0.87       157\n",
       "weighted avg       0.88      0.87      0.87       157\n",
       "\n",
       "============================================================\n",
       "\n",
       "Classification Report for decision_tree\n",
       "              precision    recall  f1-score   support\n",
       "\n",
       "    Positive       0.84      0.92      0.88        79\n",
       "    Negative       0.91      0.82      0.86        78\n",
       "\n",
       "    accuracy                           0.87       157\n",
       "   macro avg       0.88      0.87      0.87       157\n",
       "weighted avg       0.88      0.87      0.87       157\n",
       "\n",
       "============================================================\n",
       "\n",
       "Classification Report for random_forest\n",
       "              precision    recall  f1-score   support\n",
       "\n",
       "    Positive       0.84      0.92      0.88        79\n",
       "    Negative       0.91      0.82      0.86        78\n",
       "\n",
       "    accuracy                           0.87       157\n",
       "   macro avg       0.88      0.87      0.87       157\n",
       "weighted avg       0.88      0.87      0.87       157\n",
       "\n",
       "============================================================\n",
       "\n",
       "Classification Report for gradient_boosting\n",
       "              precision    recall  f1-score   support\n",
       "\n",
       "    Positive       0.84      0.92      0.88        79\n",
       "    Negative       0.91      0.82      0.86        78\n",
       "\n",
       "    accuracy                           0.87       157\n",
       "   macro avg       0.88      0.87      0.87       157\n",
       "weighted avg       0.88      0.87      0.87       157\n",
       "\n",
       "============================================================\n",
       "\n",
       "Classification Report for ada_boost\n",
       "              precision    recall  f1-score   support\n",
       "\n",
       "    Positive       0.84      0.92      0.88        79\n",
       "    Negative       0.91      0.82      0.86        78\n",
       "\n",
       "    accuracy                           0.87       157\n",
       "   macro avg       0.88      0.87      0.87       157\n",
       "weighted avg       0.88      0.87      0.87       157\n",
       "\n",
       "============================================================\n",
       "\n",
       "Classification Report for extra_trees\n",
       "              precision    recall  f1-score   support\n",
       "\n",
       "    Positive       0.84      0.92      0.88        79\n",
       "    Negative       0.91      0.82      0.86        78\n",
       "\n",
       "    accuracy                           0.87       157\n",
       "   macro avg       0.88      0.87      0.87       157\n",
       "weighted avg       0.88      0.87      0.87       157\n",
       "\n",
       "============================================================\n",
       "\n",
       "Classification Report for K_nearest_neighbor\n",
       "              precision    recall  f1-score   support\n",
       "\n",
       "    Positive       0.84      0.92      0.88        79\n",
       "    Negative       0.91      0.82      0.86        78\n",
       "\n",
       "    accuracy                           0.87       157\n",
       "   macro avg       0.88      0.87      0.87       157\n",
       "weighted avg       0.88      0.87      0.87       157\n",
       "\n",
       "============================================================\n",
       "\n",
       "Classification Report for bagging_classifier\n",
       "              precision    recall  f1-score   support\n",
       "\n",
       "    Positive       0.84      0.92      0.88        79\n",
       "    Negative       0.91      0.82      0.86        78\n",
       "\n",
       "    accuracy                           0.87       157\n",
       "   macro avg       0.88      0.87      0.87       157\n",
       "weighted avg       0.88      0.87      0.87       157\n",
       "\n",
       "============================================================\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the classification report\n",
    "\n",
    "for model_name, model_classifier in models.items():\n",
    "    \n",
    "    print(f\"Classification Report for {model_name}\")\n",
    "    print(classification_report(y_val, y_pred, target_names = [\"Positive\", \"Negative\"]))\n",
    "    print(\"==\"*30 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning using RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter grid for each model\n",
    "\n",
    "param_grids = {\n",
    "    \"logistic_regression\": {\n",
    "        'C': [0.1, 1, 10, 100],  # Regularization strength\n",
    "        'solver': ['newton-cg', 'lbfgs', 'liblinear'],  # Optimization algorithms\n",
    "        'penalty': ['l2'],  # L2 regularization\n",
    "        'max_iter': [100, 200, 500]\n",
    "    },\n",
    "    \"decision_tree\": {\n",
    "        'criterion': ['gini', 'entropy'],\n",
    "        'splitter': ['best', 'random'],\n",
    "        'max_depth': [5, 10, 20, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 5],\n",
    "        'max_features': ['auto', 'sqrt', 'log2', None]\n",
    "    },\n",
    "    \"random_forest\": {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [10, 20, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'bootstrap': [True, False],\n",
    "        'max_features': ['auto', 'sqrt', 'log2']\n",
    "    },\n",
    "    \"gradient_boosting\": {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'subsample': [0.7, 0.8, 1.0],\n",
    "        'max_depth': [3, 5, 10],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    \"ada_boost\": {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'learning_rate': [0.01, 0.1, 1.0],\n",
    "        'algorithm': ['SAMME', 'SAMME.R']\n",
    "    },\n",
    "    \"extra_trees\": {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [10, 20, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'bootstrap': [True, False]\n",
    "    },\n",
    "    \"K_nearest_neighbor\": {\n",
    "        'n_neighbors': [3, 5, 11, 19],\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'metric': ['euclidean', 'manhattan', 'minkowski']\n",
    "    },\n",
    "    \"bagging_classifier\": {\n",
    "        'n_estimators': [10, 50, 100],\n",
    "        'max_samples': [0.5, 0.7, 1.0],\n",
    "        'max_features': [0.5, 0.7, 1.0],\n",
    "        'bootstrap': [True, False]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                 model  \\\n",
       "0        random_forest   \n",
       "1    gradient_boosting   \n",
       "2          extra_trees   \n",
       "3   bagging_classifier   \n",
       "4        decision_tree   \n",
       "5            ada_boost   \n",
       "6   K_nearest_neighbor   \n",
       "7  logistic_regression   \n",
       "\n",
       "                                                                                                                           best_params  \\\n",
       "0    {'n_estimators': 100, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': 20, 'bootstrap': False}   \n",
       "1        {'subsample': 0.8, 'n_estimators': 200, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_depth': 10, 'learning_rate': 0.2}   \n",
       "2                            {'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_depth': 20, 'bootstrap': False}   \n",
       "3                                                    {'n_estimators': 100, 'max_samples': 0.7, 'max_features': 0.5, 'bootstrap': True}   \n",
       "4  {'splitter': 'best', 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': None, 'criterion': 'gini'}   \n",
       "5                                                                  {'n_estimators': 200, 'learning_rate': 1.0, 'algorithm': 'SAMME.R'}   \n",
       "6                                                                     {'weights': 'distance', 'n_neighbors': 5, 'metric': 'euclidean'}   \n",
       "7                                                                  {'solver': 'newton-cg', 'penalty': 'l2', 'max_iter': 100, 'C': 0.1}   \n",
       "\n",
       "   best_scores  \n",
       "0     0.868036  \n",
       "1     0.862798  \n",
       "2     0.860903  \n",
       "3     0.854479  \n",
       "4     0.805941  \n",
       "5     0.790272  \n",
       "6     0.774610  \n",
       "7     0.743930  \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create empty dataframe to store the results\n",
    "rand_table = pd.DataFrame(columns = [\"model\", \"best_params\", \"best_scores\"])\n",
    "\n",
    "# Initialize an empty dictionary to hold the RandomizedSearchCV results for each model\n",
    "rand_searches_tuned = {}\n",
    "\n",
    "# Initialize a dictionary to hold the best model\n",
    "best_model_rand = {}\n",
    "\n",
    "# Create a stratified object\n",
    "skf = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 42)\n",
    "\n",
    "# Iterate over models and apply RandomizedSearchCV for hyperparameter tuning\n",
    "\n",
    "for model_name, model_classifier in models.items():\n",
    "    \n",
    "    # Get the parameter grid for the model\n",
    "    param_grid = param_grids.get(model_name, {})\n",
    "    \n",
    "    # Set up the RandomizedSearchCV with 5-fold cross-validation\n",
    "    rand_search_tuned = RandomizedSearchCV(\n",
    "        model_classifier,\n",
    "        param_distributions=param_grid,\n",
    "        n_iter=20,\n",
    "        cv=skf,\n",
    "        scoring='f1',\n",
    "        verbose = 0,\n",
    "        n_jobs =- 1,\n",
    "        random_state = 42\n",
    "    )\n",
    "    \n",
    "    # Fit RandomizedSearchCV\n",
    "    rand_search_tuned.fit(X_train_processed, y_train)\n",
    "    \n",
    "    # Store the randomized search object in the dictionary\n",
    "    rand_searches_tuned[model] = rand_search_tuned\n",
    "    \n",
    "    # Add the results to the dataframe\n",
    "    best_params = rand_search_tuned.best_params_\n",
    "    best_score = rand_search_tuned.best_score_\n",
    "    \n",
    "    rand_table.loc[len(rand_table)] = [model_name, best_params, best_score]\n",
    "    \n",
    "    # Store the best model in best_model_rand dictionary\n",
    "    best_model_rand[model_name] = rand_search_tuned.best_estimator_\n",
    "    \n",
    "    \n",
    "# Sort the rand table with the best score\n",
    "rand_table = rand_table.sort_values(by = \"best_scores\", ascending = False).reset_index(drop = True)\n",
    "\n",
    "# Display the rand table\n",
    "print(rand_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of test data set\n",
    "test_df = test_data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis (EDA) of Test Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'pandas.core.frame.DataFrame'>\n",
       "RangeIndex: 169 entries, 0 to 168\n",
       "Data columns (total 10 columns):\n",
       " #   Column     Non-Null Count  Dtype  \n",
       "---  ------     --------------  -----  \n",
       " 0   ID         169 non-null    object \n",
       " 1   PRG        169 non-null    int64  \n",
       " 2   PL         169 non-null    int64  \n",
       " 3   PR         169 non-null    int64  \n",
       " 4   SK         169 non-null    int64  \n",
       " 5   TS         169 non-null    int64  \n",
       " 6   M11        169 non-null    float64\n",
       " 7   BD2        169 non-null    float64\n",
       " 8   Age        169 non-null    int64  \n",
       " 9   Insurance  169 non-null    int64  \n",
       "dtypes: float64(2), int64(7), object(1)\n",
       "memory usage: 13.3+ KB\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check the info of the test data\n",
    "\n",
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID           0\n",
       "PRG          0\n",
       "PL           0\n",
       "PR           0\n",
       "SK           0\n",
       "TS           0\n",
       "M11          0\n",
       "BD2          0\n",
       "Age          0\n",
       "Insurance    0\n",
       "dtype: int64\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check for missing values of the test set\n",
    "\n",
    "test_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  Patient_ID  Plasma_Glucose  Blood_Work_Result-1 (mu U/ml)  \\\n",
       "0  ICU200609               1                            109   \n",
       "1  ICU200610               1                            108   \n",
       "2  ICU200611               6                             96   \n",
       "3  ICU200612               1                            124   \n",
       "4  ICU200613               7                            150   \n",
       "\n",
       "   Blood_Pressure (mm Hg)  Blood_Work_Result-2 (mm)  \\\n",
       "0                      38                        18   \n",
       "1                      88                        19   \n",
       "2                       0                         0   \n",
       "3                      74                        36   \n",
       "4                      78                        29   \n",
       "\n",
       "   Blood_Work_Result-3 (mu U/ml)  \\\n",
       "0                            120   \n",
       "1                              0   \n",
       "2                              0   \n",
       "3                              0   \n",
       "4                            126   \n",
       "\n",
       "   Body_Mass_Index (weight in kg/(height in m)^2  \\\n",
       "0                                           23.1   \n",
       "1                                           27.1   \n",
       "2                                           23.7   \n",
       "3                                           27.8   \n",
       "4                                           35.2   \n",
       "\n",
       "   Blood_Work_Result-4 (mu U/ml)  Age  Insurance  \n",
       "0                          0.407   26          1  \n",
       "1                          0.400   24          1  \n",
       "2                          0.190   28          1  \n",
       "3                          0.100   30          1  \n",
       "4                          0.692   54          0  \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Rename the columns to give descriptive meaning\n",
    "\n",
    "cols = {\n",
    "    'ID': 'Patient_ID',\n",
    "    'PRG': 'Plasma_Glucose',\n",
    "    'PL': 'Blood_Work_Result-1 (mu U/ml)',\n",
    "    'PR': 'Blood_Pressure (mm Hg)',\n",
    "    'SK': 'Blood_Work_Result-2 (mm)',\n",
    "    'TS': 'Blood_Work_Result-3 (mu U/ml)',\n",
    "    'M11': 'Body_Mass_Index (weight in kg/(height in m)^2',\n",
    "    'BD2': 'Blood_Work_Result-4 (mu U/ml)',\n",
    "    'Sepssis' : 'Sepsis'\n",
    "}\n",
    "\n",
    "test_df.rename(columns = cols, inplace = True)\n",
    "\n",
    "# Check the data\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Patient_ID column is of object data type\n",
       "\n",
       "np.isnan() is not applicable to Patient_ID column\n",
       "Plasma_Glucose column is of int64 data type\n",
       "\n",
       "Blood_Work_Result-1 (mu U/ml) column is of int64 data type\n",
       "\n",
       "Blood_Pressure (mm Hg) column is of int64 data type\n",
       "\n",
       "Blood_Work_Result-2 (mm) column is of int64 data type\n",
       "\n",
       "Blood_Work_Result-3 (mu U/ml) column is of int64 data type\n",
       "\n",
       "Body_Mass_Index (weight in kg/(height in m)^2 column is of float64 data type\n",
       "\n",
       "Blood_Work_Result-4 (mu U/ml) column is of float64 data type\n",
       "\n",
       "Age column is of int64 data type\n",
       "\n",
       "Insurance column is of int64 data type\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Thorough check the test set\n",
    "\n",
    "for col in test_df.columns:\n",
    "    print(f\"{col} column is of {test_df[col].dtype} data type\\n\")\n",
    "    \n",
    "    # Check if applying np.isnan will work\n",
    "    try:\n",
    "        np.isnan(test_df[col])\n",
    "    except TypeError:\n",
    "        print(f\"np.isnan() is not applicable to {col} column\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the unnecessary columns from the test_set dataframe\n",
    "\n",
    "test_set = test_df.drop(columns = [\"Patient_ID\"], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Model on the Test Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dictionary to store the best estimators\n",
    "best_estimators = {}\n",
    "\n",
    "# Create an empty dictionary to store the y tests results\n",
    "y_test = {}\n",
    "\n",
    "# Iterate over the top four best models\n",
    "for model in ['gradient_boosting', 'random_forest', 'extra_trees', 'bagging_classifier']:\n",
    "    if model in best_model_rand:\n",
    "        best_estimators[model] = best_model_rand[model]\n",
    "    else:\n",
    "        print(f\"{model} does not exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model, model in best_estimators.items():\n",
    "    y_test[model] = model.predict(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1 0 0 0 1 0 0 1 1 1 1 1 1 1 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0\n",
       " 0 1 1 1 1 0 0 0 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 1 0 0 1 1 0 1 0 0 1 1 1 0 1\n",
       " 1 0 0 0 0 0 1 1 0 1 0 0 1 0 0 1 1 0 0 1 1 0 1 1 0 1 0 1 0 0 0 1 0 0 1 0 1\n",
       " 1 1 0 1 0 1 1 0 1 0 0 1 1 1 0 0 1 0 0 0 1 0 1 1 0 0 1 0 1 0 1 1 1 0 1 1 0\n",
       " 1 1 0 0 1 0 1 0 1 0 0 0 0 1 0 0 1 0 1 0 0]\n",
       "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
       " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
       " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
       " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
       " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
       "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
       " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
       " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
       " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
       " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
       "[0 1 1 1 0 1 1 0 0 0 0 0 0 0 0 0 1 1 0 1 1 0 1 1 0 1 0 1 1 1 1 1 0 1 0 1 1\n",
       " 1 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 1 1 0 0 0 1 0\n",
       " 0 1 0 1 1 1 0 0 0 0 1 1 0 1 1 0 0 0 1 0 0 1 0 0 1 0 1 0 1 1 0 0 1 1 0 1 0\n",
       " 0 0 1 0 1 0 0 1 0 0 1 0 0 0 1 1 0 1 1 1 0 1 0 0 1 1 0 1 0 1 0 0 0 1 0 0 1\n",
       " 0 0 1 0 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0 1 1]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Print the predictions for each model\n",
    "for model_name, predictions in y_test.items():\n",
    "    print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model Scores Evaluations on Test Data:\n",
       "                   accuracy_score f1_score precision_score recall_score  \\\n",
       "bagging_classifier            1.0      1.0             1.0          1.0   \n",
       "random_forest            0.544379      0.0             0.0          0.0   \n",
       "gradient_boosting        0.047337      0.0             0.0          0.0   \n",
       "extra_trees              0.544379      0.0             0.0          0.0   \n",
       "\n",
       "                      confusion_matrix roc_auc_score  \n",
       "bagging_classifier  [[92, 0], [0, 77]]           1.0  \n",
       "random_forest       [[92, 0], [77, 0]]           0.5  \n",
       "gradient_boosting   [[8, 84], [77, 0]]      0.043478  \n",
       "extra_trees         [[92, 0], [77, 0]]           0.5  \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import the confusion matrix for use so that it can be updated\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Initialize Results dictionary\n",
    "result_test = {}\n",
    "\n",
    "# Loop through rand_searches_tuned items\n",
    "for model, rand_search_tuned in best_model_rand.items():\n",
    "    \n",
    "    # Ensure only top four best performing models are processed\n",
    "    if model not in [\"gradient_boosting\", \"random_forest\",  \"extra_trees\", \"bagging_classifier\"]:\n",
    "        continue\n",
    "    \n",
    "    # Predict on evaluation data\n",
    "    y_test_pred = rand_search_tuned.predict(test_set)\n",
    "    \n",
    "    # Calculate other metrics\n",
    "    accuracy = accuracy_score(predictions, y_test_pred)\n",
    "    f1 = f1_score(predictions, y_test_pred)\n",
    "    precision = precision_score(predictions, y_test_pred)\n",
    "    recall = recall_score(predictions, y_test_pred)\n",
    "    conf_matrix = confusion_matrix(predictions, y_test_pred)\n",
    "    # y_test_prob = rand_search_tuned(predictions, y_test_pred)\n",
    "    roc_auc = roc_auc_score(predictions, y_test_pred)\n",
    "    \n",
    "    # Store the results in eval_results_tuned dictionary\n",
    "    result_test[model] = {\n",
    "        'accuracy_score' : accuracy,\n",
    "        'f1_score' : f1,\n",
    "        'precision_score' : precision,\n",
    "        'recall_score' : recall,\n",
    "        'confusion_matrix' : conf_matrix,\n",
    "        # 'y_test_prob' : y_test_prob,\n",
    "        'roc_auc_score' : roc_auc\n",
    "    }\n",
    "    \n",
    "# Convert results into a dataframe\n",
    "scores_test = pd.DataFrame(result_test).transpose()\n",
    "\n",
    "# Sort the dataframe by roc_auc column\n",
    "scores_test_df = scores_test.sort_values(by = \"f1_score\", ascending = False)\n",
    "\n",
    "# Display the sorted evaluation scores\n",
    "print(\"Model Scores Evaluations on Test Data:\")\n",
    "print(scores_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a visualization of the test confusion matrix\n",
    "\n",
    "for model in result_test.keys():\n",
    "    conf_mat = result_test[model][\"confusion_matrix\"]\n",
    "    confusion_matrix_plot(conf_mat, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:**\n",
    "\n",
    "The created model helps the medical facilities to predict Sepsis disease, which will enhance actions to have early detection of patients who can potentially contract the diease and, consequently, reduce the overall ineffcetion rates. Since Sepsis is caused by a range of factors, one has to focus on them, for instance, patients can be having regular medical check-ups for their blood pressure and body mass index among others. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the Cleaned Data, Model and Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving my clean data frame 'traindata'\n",
    "\n",
    "train_data.to_csv('train_sepsis_data.csv', index=False)\n",
    "test_set.to_csv('test_sepsis_data.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                 model  \\\n",
       "0        random_forest   \n",
       "1    gradient_boosting   \n",
       "2          extra_trees   \n",
       "3   bagging_classifier   \n",
       "4        decision_tree   \n",
       "5            ada_boost   \n",
       "6   K_nearest_neighbor   \n",
       "7  logistic_regression   \n",
       "\n",
       "                                                                                                                           best_params  \\\n",
       "0    {'n_estimators': 100, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': 20, 'bootstrap': False}   \n",
       "1        {'subsample': 0.8, 'n_estimators': 200, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_depth': 10, 'learning_rate': 0.2}   \n",
       "2                            {'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_depth': 20, 'bootstrap': False}   \n",
       "3                                                    {'n_estimators': 100, 'max_samples': 0.7, 'max_features': 0.5, 'bootstrap': True}   \n",
       "4  {'splitter': 'best', 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': None, 'criterion': 'gini'}   \n",
       "5                                                                  {'n_estimators': 200, 'learning_rate': 1.0, 'algorithm': 'SAMME.R'}   \n",
       "6                                                                     {'weights': 'distance', 'n_neighbors': 5, 'metric': 'euclidean'}   \n",
       "7                                                                  {'solver': 'newton-cg', 'penalty': 'l2', 'max_iter': 100, 'C': 0.1}   \n",
       "\n",
       "   best_scores  \n",
       "0     0.868036  \n",
       "1     0.862798  \n",
       "2     0.860903  \n",
       "3     0.854479  \n",
       "4     0.805941  \n",
       "5     0.790272  \n",
       "6     0.774610  \n",
       "7     0.743930  \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the dataframe that have scored the hyperparametered models\n",
    "print(rand_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save each model pipeline as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Saved logistic_regression pipeline to logistic_regression_pipeline.pkl\n",
       "Saved decision_tree pipeline to decision_tree_pipeline.pkl\n",
       "Saved random_forest pipeline to random_forest_pipeline.pkl\n",
       "Saved gradient_boosting pipeline to gradient_boosting_pipeline.pkl\n",
       "Saved ada_boost pipeline to ada_boost_pipeline.pkl\n",
       "Saved extra_trees pipeline to extra_trees_pipeline.pkl\n",
       "Saved K_nearest_neighbor pipeline to K_nearest_neighbor_pipeline.pkl\n",
       "Saved bagging_classifier pipeline to bagging_classifier_pipeline.pkl\n",
       "Saved logistic_regression model to logistic_regression_model.pkl\n",
       "Saved decision_tree model to decision_tree_model.pkl\n",
       "Saved random_forest model to random_forest_model.pkl\n",
       "Saved gradient_boosting model to gradient_boosting_model.pkl\n",
       "Saved ada_boost model to ada_boost_model.pkl\n",
       "Saved extra_trees model to extra_trees_model.pkl\n",
       "Saved K_nearest_neighbor model to K_nearest_neighbor_model.pkl\n",
       "Saved bagging_classifier model to bagging_classifier_model.pkl\n",
       "[1 1 1 1 0 0 1 0 1 1 1 1 0 0 1 0 1 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1\n",
       " 1 1 1 1 1 0 1 1 1 0 0 0 1 1 1 1 1 1 1 0 1 0 1 1 1 0 0 0 1 1 1 1 1 0 0 1 1\n",
       " 0 1 0 1 1 1 1 1 0 1 1 1 0 1 1 1 0 1 0 1 0 1 0 0 1 1 0 1 1 0 0 1 1 1 1 0 1\n",
       " 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 0 1\n",
       " 1 0 1 0 1 1 0 0 0 1 0 1 1 1 0 1 1 1 1 0 1]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create and save the pipelines for the best models\n",
    "model_pipelines = {}\n",
    "\n",
    "for model_name, best_model in best_model_rand.items():\n",
    "    # Combine preprocessor and model into a pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),  # Add the preprocessor\n",
    "        ('classifier', best_model)  # Add the best model\n",
    "    ])\n",
    "    \n",
    "    # Fit the pipeline on the training data\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Save the pipeline as a pickle file\n",
    "    pipeline_filename = f\"{model_name}_pipeline.pkl\"\n",
    "    with open(pipeline_filename, 'wb') as file:\n",
    "        pickle.dump(pipeline, file)\n",
    "    \n",
    "    # Store the pipeline for further use\n",
    "    model_pipelines[model_name] = pipeline\n",
    "    print(f\"Saved {model_name} pipeline to {pipeline_filename}\")\n",
    "\n",
    "# Save individual models (optional, since pipelines include models)\n",
    "for model_name, model in best_model_rand.items():\n",
    "    model_filename = f\"{model_name}_model.pkl\"\n",
    "    joblib.dump(model, model_filename)\n",
    "    print(f\"Saved {model_name} model to {model_filename}\")\n",
    "\n",
    "# Loading the pipelines or models for inference\n",
    "# Example: Loading a saved pipeline\n",
    "pipeline_path = \"gradient_boosting_pipeline.pkl\"\n",
    "with open(pipeline_path, 'rb') as file:\n",
    "    loaded_pipeline = pickle.load(file)\n",
    "\n",
    "# Predict using the loaded pipeline\n",
    "test_predictions = loaded_pipeline.predict(test_set)\n",
    "print(test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
